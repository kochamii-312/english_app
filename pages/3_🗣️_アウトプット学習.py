# pages/3_ğŸ—£ï¸_ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆç·´ç¿’.py
import io
import json
import os
import time
import wave
import datetime as dt
import streamlit as st
import base64
import random
import hashlib
# from dotenv import load_dotenv
from openai import OpenAI
from st_audiorec import st_audiorec  # pip install streamlit-audiorec

# =============== åŸºæœ¬è¨­å®š ===============
st.set_page_config(page_title="ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆç·´ç¿’", layout="wide")
st.markdown("## ğŸ—£ï¸ ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆç·´ç¿’")

# .env â†’ OPENAI_API_KEY
# load_dotenv()
# client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# ä¿å­˜å…ˆï¼ˆéŒ²éŸ³éŸ³å£° / ãƒ­ã‚°JSONï¼‰
RECORDINGS_DIR_CANDIDATES = [
    "./recordings",
    os.path.join(os.path.dirname(__file__), "recordings"),
    "/mnt/data/recordings",
]
LOG_JSON_CANDIDATES = [
    "./json/recordings_output.json",                               # æ–°ã—ã„éŒ²éŸ³ãƒ­ã‚°ï¼ˆå‡ºåŠ›è¦ä»¶ï¼‰
    os.path.join(os.path.dirname(__file__), "./json/recordings_output.json"),
    "/mnt/data/json/recordings_output.json",
]
OUTPUT_JSON_PATHS = [
    "./json/output.json",                                          # æ—¢å­˜ã®å‡ºé¡Œä¿å­˜ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰ã«å¯¾å¿œ
    os.path.join(os.path.dirname(__file__), "./json/output.json"),
    "/mnt/data/json/output.json",
]
DESC_IMG_DIR_CANDIDATES = [
    "./desc_images",
    os.path.join(os.path.dirname(__file__), "desc_images"),
    "/mnt/data/desc_images",
]

def ensure_dir() -> str:
    for d in RECORDINGS_DIR_CANDIDATES:
        try:
            os.makedirs(d, exist_ok=True)
            # æ›¸ãè¾¼ã¿æ¤œè¨¼
            p = os.path.join(d, ".touch")
            with open(p, "w", encoding="utf-8") as f:
                f.write("")
            os.remove(p)
            return d
        except Exception:
            continue
    os.makedirs("./recordings", exist_ok=True)
    return "./recordings"

def resolve_path(cands) -> str:
    # æ—¢å­˜ or ä½œæˆå¯èƒ½ãªæœ€åˆã®ãƒ‘ã‚¹ã‚’è¿”ã™
    for p in cands:
        parent = os.path.dirname(p) or "."
        if os.path.exists(p) or os.path.isdir(parent):
            return p
    return cands[0]

RECORDING_DIR = ensure_dir()
RECORD_LOG_PATH = resolve_path(LOG_JSON_CANDIDATES)
OUTPUT_JSON_PATH = resolve_path(OUTPUT_JSON_PATHS)

# =============== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ===============
def save_bytes_to_file(b: bytes, path: str) -> None:
    with open(path, "wb") as f:
        f.write(b)

def compute_wav_duration_seconds(wav_bytes: bytes) -> float:
    # st_audiorec ã¯ WAV ãƒ˜ãƒƒãƒ€ä»˜ä¸æ¸ˆã¿
    with wave.open(io.BytesIO(wav_bytes), "rb") as wf:
        frames = wf.getnframes()
        rate = wf.getframerate()
        return float(frames) / float(rate) if rate else 0.0

def append_json(path: str, item: dict) -> None:
    data = []
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception:
            data = []
    data.append(item)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# =============== å‡ºé¡Œç”Ÿæˆï¼ˆä»»æ„ï¼‰ ===============
def generate_toefl_question() -> str:
    """TOEFLãƒ¬ãƒ™ãƒ«ã®ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³è³ªå•ã‚’1ã¤ç”Ÿæˆï¼ˆå¿…è¦ãªã‚‰ output.json ã«ã‚‚ä¿å­˜å¯èƒ½ï¼‰ã€‚"""
    prompt = """Generate ONE TOEFL-style discussion question (B2â€“C1) in one sentence.
Return only the question text."""
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0.7,
        messages=[
            {"role": "system", "content": "You create concise TOEFL-style prompts."},
            {"role": "user", "content": prompt}
        ]
    )
    q = resp.choices[0].message.content.strip().strip('"')
    # æ—¢å­˜æ–¹é‡ï¼šoutput.json ã«ã‚‚ä¿å­˜ã—ã¦ãŠãï¼ˆå¿…è¦ãªã‚‰ï¼‰
    try:
        # { "key": int, "question": "..." } ã®é…åˆ—æƒ³å®šï¼ˆã‚µãƒ³ãƒ—ãƒ«æº–æ‹ ï¼‰
        # æ–°è¦ key ã¯é€£ç•ªã§æ¡ç•ª
        dat = []
        if os.path.exists(OUTPUT_JSON_PATH):
            with open(OUTPUT_JSON_PATH, "r", encoding="utf-8") as f:
                dat = json.load(f)
        next_key = (max([d.get("key", 0) for d in dat]) + 1) if dat else 1
        dat.append({"key": next_key, "question": q})
        with open(OUTPUT_JSON_PATH, "w", encoding="utf-8") as f:
            json.dump(dat, f, ensure_ascii=False, indent=2)
    except Exception:
        pass
    return q

# =============== æ–‡å­—èµ·ã“ã— & è©•ä¾¡ ===============
def transcribe_wav_bytes(wav_bytes: bytes) -> str:
    """OpenAI Whisperã§æ–‡å­—èµ·ã“ã—ï¼ˆè¦APIã‚­ãƒ¼ï¼‰ã€‚"""
    # OpenAI API ã¯ãƒ•ã‚¡ã‚¤ãƒ«likeã‚’å—ã‘ä»˜ã‘ã‚‹
    audio_f = io.BytesIO(wav_bytes)
    audio_f.name = "audio.wav"
    r = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_f
    )
    return r.text.strip()

def evaluate_speaking(question: str, transcript: str) -> dict:
    """æ–‡æ³•/å†…å®¹/æµæš¢ã•ã‚’5ç‚¹æº€ç‚¹ã§è©•ä¾¡ã—ã€çŸ­ã„è¬›è©•ã¨æ”¹å–„ææ¡ˆã‚’JSONã§è¿”ã™ã€‚"""
    user = f"""
Question: {question}
Learner's response (transcribed): {transcript}

Rate on 0â€“5 scale:
- grammar
- content_relevance
- fluency

Return strict JSON:
{{
  "scores": {{"grammar": <int>, "content_relevance": <int>, "fluency": <int>}},
  "comment": "<one-paragraph natural feedback>",
  "tips": ["<short fix 1>", "<short fix 2>", "<short fix 3>"]
}}
"""
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0.4,
        messages=[
            {"role": "system", "content": "You are a precise English speaking evaluator for TOEFL practice."},
            {"role": "user", "content": user}
        ]
    )
    content = resp.choices[0].message.content
    # JSONæŠ½å‡º
    start = content.find("{"); end = content.rfind("}")
    if start != -1 and end != -1:
        content = content[start:end+1]
    return json.loads(content)

# =============== éŒ²éŸ³ãƒãƒ³ãƒ‰ãƒ© ===============
def handle_recording(category: str, mode: str, question: str, wav_audio_data: bytes, image_file: str | None = None):
    """éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜â†’é•·ã•è¨ˆç®—â†’æ–‡å­—èµ·ã“ã—â†’è©•ä¾¡â†’éŒ²éŸ³ãƒ­ã‚°JSONã¸è¿½è¨˜ã€‚"""
    ts = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    base = f"{category}_{mode}_{ts}"
    audio_path = os.path.join(RECORDING_DIR, f"{base}.wav")

    # 1) ä¿å­˜
    save_bytes_to_file(wav_audio_data, audio_path)

    # 2) é•·ã•
    dur = compute_wav_duration_seconds(wav_audio_data)

    # 3) æ–‡å­—èµ·ã“ã—
    transcript = ""
    try:
        transcript = transcribe_wav_bytes(wav_audio_data)
    except Exception as e:
        transcript = ""
        st.warning(f"æ–‡å­—èµ·ã“ã—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    # 4) è©•ä¾¡
    evaluation = {}
    try:
        evaluation = evaluate_speaking(question, transcript or "(no transcript)")
    except Exception as e:
        evaluation = {"scores": {"grammar": 0, "content_relevance": 0, "fluency": 0},
                      "comment": f"Evaluation failed: {e}", "tips": []}

    # 5) ãƒ­ã‚°JSONã¸è¿½è¨˜ï¼ˆç”»åƒãƒ‘ã‚¹ã‚‚å«ã‚ã‚‹ï¼‰
    log_item = {
        "timestamp": ts,
        "category": "output",            # è¦ä»¶ã©ãŠã‚Šå›ºå®š
        "mode": mode,                    # "discussion" / "description"
        "question": question,
        "audio_file": audio_path,
        "duration_sec": round(dur, 2),
        "transcript": transcript,
        "evaluation": evaluation
    }
    if image_file:
        log_item["image_file"] = image_file

    append_json(RECORD_LOG_PATH, log_item)

    # 6) ç”»é¢è¡¨ç¤º
    st.success("éŒ²éŸ³ãƒ»ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    st.audio(audio_path)
    if image_file:
        st.image(image_file, caption="Saved picture for DESCRIPTION", use_container_width=True)
    st.json({
        "saved_to": RECORD_LOG_PATH,
        "duration_sec": round(dur, 2),
        "evaluation_summary": evaluation.get("scores", {})
    })
    # ğŸ‘‰ è¿½åŠ ï¼šå‘¼ã³å‡ºã—å´ã§è©³ç´°è¡¨ç¤ºã§ãã‚‹ã‚ˆã†è¿”ã™
    return {
        "audio_path": audio_path,
        "image_path": image_file,
        "duration_sec": round(dur, 2),
        "transcript": transcript,
        "evaluation": evaluation,
        "log_path": RECORD_LOG_PATH,
    }

def _ensure_desc_img_dir() -> str:
    for d in DESC_IMG_DIR_CANDIDATES:
        try:
            os.makedirs(d, exist_ok=True)
            # æ›¸ãè¾¼ã¿æ¤œè¨¼
            p = os.path.join(d, ".touch")
            with open(p, "w", encoding="utf-8") as f:
                f.write("")
            os.remove(p)
            return d
        except Exception:
            continue
    os.makedirs("./desc_images", exist_ok=True)
    return "./desc_images"

DESC_IMG_DIR = _ensure_desc_img_dir()

def _description_categories() -> list[str]:
    # å¤šæ§˜ãªæ—¥å¸¸/å…¬å…±ã‚·ãƒ¼ãƒ³
    return [
        "modern open-plan office",
        "cozy living room at home",
        "airplane cabin during boarding",
        "busy train station concourse",
        "university classroom with projector",
        "quiet library interior",
        "coffee shop counter and seating area",
        "suburban kitchen interior",
        "hotel lobby reception area",
        "tech startup workspace with whiteboards"
    ]

def generate_description_image(client: OpenAI) -> tuple[str, str]:
    """
    èª¬æ˜ã‚¿ã‚¹ã‚¯ç”¨ã®å†™çœŸã‚’1æšç”Ÿæˆã—ã¦ä¿å­˜ã€‚
    æˆ»ã‚Šå€¤: (image_path, prompt_used)
    """
    category = random.choice(_description_categories())
    prompt = (
        f"Photorealistic, high-resolution interior or public scene: {category}. "
        "Natural lighting, realistic textures, rich details. No text or watermarks."
    )

    # OpenAI Images APIï¼ˆbase64ã§å—ã‘å–ã‚ŠPNGä¿å­˜ï¼‰
    img_resp = client.images.generate(
        model="gpt-image-1",
        prompt=prompt,
        size="1024x1024",
        quality="high",
        n=1,
    )
    b64 = img_resp.data[0].b64_json
    img_bytes = base64.b64decode(b64)

    ts = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    out_path = os.path.join(DESC_IMG_DIR, f"description_{ts}.png")
    with open(out_path, "wb") as f:
        f.write(img_bytes)

    return out_path, prompt

# ãƒ¢ãƒ¼ãƒ‰é¸æŠï¼ˆDISCUSSION / DESCRIPTIONï¼‰
mode = st.radio("ãƒ¢ãƒ¼ãƒ‰ã‚’é¸ã‚“ã§ãã ã•ã„", ["DISCUSSION", "DESCRIPTION"], horizontal=True)

if "discussion_q" not in st.session_state:
    st.session_state.discussion_q = "What is your hobby?"
if "auto_eval" not in st.session_state:
    st.session_state.auto_eval = True
if "last_audio_fingerprint" not in st.session_state:
    st.session_state.last_audio_fingerprint = None

def _fingerprint_audio(b: bytes) -> str:
    return hashlib.md5(b).hexdigest()

if mode == "DISCUSSION":
    st.subheader("Question")
    colq1, colq2 = st.columns([3,1])
    with colq1:
        st.write(st.session_state.discussion_q)
    with colq2:
        if st.button("ğŸ”„ è¨­å•ã‚’ç”Ÿæˆ"):
            st.session_state.discussion_q = generate_toefl_question()
            st.rerun()
    current_question = st.session_state.discussion_q
    current_image_path = None  # DISCUSSIONã§ã¯ç”»åƒãªã—

else:
    st.subheader("Description Task")
    st.caption("ã‚ªãƒ•ã‚£ã‚¹/å®¶/é£›è¡Œæ©Ÿ/é§…ãªã©ã€æ—¥å¸¸ã‚„å…¬å…±ã®ã‚·ãƒ¼ãƒ³å†™çœŸãŒè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã™ã€‚1åˆ†ã»ã©ã§æ˜å¿«ã«èª¬æ˜ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚")
    # ç”»åƒã®è‡ªå‹•ç”Ÿæˆï¼ˆåˆå›ã®ã¿ï¼‰
    if "desc_image_path" not in st.session_state or st.session_state.desc_image_path is None:
        with st.spinner("å†™çœŸã‚’ç”Ÿæˆä¸­..."):
            try:
                img_path, used_prompt = generate_description_image(client)
                st.session_state.desc_image_path = img_path
                st.session_state.desc_image_prompt = used_prompt
            except Exception as e:
                st.error(f"ç”»åƒç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                st.session_state.desc_image_path = None
                st.session_state.desc_image_prompt = None

    # å†ç”Ÿæˆãƒœã‚¿ãƒ³
    regen = st.button("ğŸ”„ åˆ¥ã®ç”»åƒã‚’ç”Ÿæˆ")
    if regen:
        with st.spinner("å†™çœŸã‚’ç”Ÿæˆä¸­..."):
            try:
                img_path, used_prompt = generate_description_image(client)
                st.session_state.desc_image_path = img_path
                st.session_state.desc_image_prompt = used_prompt
            except Exception as e:
                st.error(f"ç”»åƒç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    # è¡¨ç¤º
    current_image_path = st.session_state.get("desc_image_path")
    if current_image_path:
        st.image(current_image_path, caption="Generated picture", use_container_width=True)
        with st.expander("ç”»åƒç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå‚è€ƒï¼‰"):
            st.code(st.session_state.get("desc_image_prompt", ""), language="text")

    # è©•ä¾¡ç”¨ã® â€œæ“¬ä¼¼è³ªå•æ–‡â€
    current_question = "Describe the given picture or situation in about one minute with clear structure."

st.divider()
st.caption("â€» éŒ²éŸ³ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã¯ãƒšãƒ¼ã‚¸ã«1ã¤ã ã‘ã§ã™ã€‚ãƒ¢ãƒ¼ãƒ‰ã‚’åˆ‡ã‚Šæ›¿ãˆã¦ä½¿ã£ã¦ãã ã•ã„ã€‚")

st.session_state.auto_eval = st.checkbox(
    "éŒ²éŸ³åœæ­¢ã—ãŸã‚‰è‡ªå‹•ã§æ¡ç‚¹ã™ã‚‹", value=st.session_state.auto_eval
)

# éŒ²éŸ³ï¼ˆå”¯ä¸€ã®st_audiorecï¼‰
wav_audio_data = st_audiorec()

if wav_audio_data is not None:
    st.subheader("éŒ²éŸ³ã—ãŸéŸ³å£°")
    st.audio(wav_audio_data, format="audio/wav")

    # â˜… ã“ã“ã§è‡ªå‹•æ¡ç‚¹ï¼šæ–°ã—ã„éŒ²éŸ³ãªã‚‰ä¸€åº¦ã ã‘èµ°ã‚‰ã›ã‚‹
    fp = _fingerprint_audio(wav_audio_data)
    if st.session_state.auto_eval and fp != st.session_state.last_audio_fingerprint:
        result = handle_recording(
            category="output",
            mode=mode.lower(),                       # "discussion"/"description"
            question=current_question,
            wav_audio_data=wav_audio_data,
            image_file=current_image_path if mode == "DESCRIPTION" else None
        )
        st.session_state.last_audio_fingerprint = fp

        # æ—¢å­˜ã®è©•ä¾¡è¡¨ç¤ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒœã‚¿ãƒ³ã¨åŒã˜è¡¨ç¤ºï¼‰ã‚’æµç”¨ã—ãŸã„å ´åˆã¯ã“ã®ä¸‹ã«è»¢è¨˜ã—ã¦OK
        if result and "evaluation" in result:
            ev = result["evaluation"]
            scores = ev.get("scores", {})
            grammar = int(scores.get("grammar", 0))
            content_rel = int(scores.get("content_relevance", 0))
            fluency = int(scores.get("fluency", 0))

            st.markdown("### ğŸ“ è©•ä¾¡çµæœï¼ˆè‡ªå‹•ï¼‰")
            m1, m2, m3 = st.columns(3)
            with m1: st.metric("Grammar", grammar)
            with m2: st.metric("Content Relevance", content_rel)
            with m3: st.metric("Fluency", fluency)

            st.progress(min((grammar/5), 1.0))
            st.progress(min((content_rel/5), 1.0))
            st.progress(min((fluency/5), 1.0))

            st.markdown("#### ç·è©•")
            st.write(ev.get("comment", ""))

            tips = ev.get("tips", [])
            if tips:
                st.markdown("#### æ”¹å–„ã®ãƒ’ãƒ³ãƒˆ")
                for t in tips:
                    st.write(f"- {t}")

            st.markdown("#### è£œåŠ©æƒ…å ±")
            st.write(f"- éŸ³å£°ã®é•·ã•: **{result.get('duration_sec', 0)} sec**")
            if result.get("transcript"):
                with st.expander("æ–‡å­—èµ·ã“ã—ã‚’è¡¨ç¤º"):
                    st.write(result["transcript"])
            st.caption(f"ä¿å­˜å…ˆ: `{result.get('log_path')}`")

    # æ‰‹å‹•ã§ã‚‚èµ°ã‚‰ã›ãŸã„äººå‘ã‘ã®ãƒœã‚¿ãƒ³ã¯æ®‹ã™ï¼ˆå¾“æ¥é€šã‚Šï¼‰
    c1, c2, _ = st.columns([1,1,2])
    with c1:
        if st.button("ğŸ’¾ ä¿å­˜ã—ã¦AIè©•ä¾¡"):
            result = handle_recording(
                category="output",
                mode=mode.lower(),
                question=current_question,
                wav_audio_data=wav_audio_data,
                image_file=current_image_path if mode == "DESCRIPTION" else None
            )
            # æ‰‹å‹•å®Ÿè¡Œã§ã‚‚æŒ‡ç´‹ã‚’æ›´æ–°ã—ã¦äºŒé‡å®Ÿè¡Œã‚’é˜²ã
            st.session_state.last_audio_fingerprint = _fingerprint_audio(wav_audio_data)

    with c2:
        st.download_button("â¬‡ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", wav_audio_data,
                           file_name="recorded_voice.wav", mime="audio/wav")
